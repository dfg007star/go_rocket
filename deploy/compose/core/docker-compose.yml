services: # Раздел, в котором описываются все контейнеры общей инфраструктуры для всех микросервисов

  # ===================================================================
  # OPENTELEMETRY COLLECTOR - ЦЕНТРАЛЬНЫЙ АГЕНТ СБОРА ЛОГОВ
  # ===================================================================
  #
  # OpenTelemetry Collector принимает логи от приложений, обрабатывает их
  # (добавляет метаданные, группирует в батчи) и отправляет в Elasticsearch.
  # Это промежуточный слой между приложениями и системой хранения.

  otel-collector:
    # Официальный образ OTel Collector с дополнительными компонентами
    # contrib версия включает экспортеры для разных систем (Elasticsearch, Prometheus, etc.)
    image: otel/opentelemetry-collector-contrib:0.123.0

    container_name: otel-collector

    # КОМАНДА ЗАПУСКА: указываем путь к файлу конфигурации
    command: [ "--config=/etc/otel-collector-config.yaml" ]

    volumes:
      # МОНТИРОВАНИЕ КОНФИГУРАЦИИ: подключаем локальный файл в контейнер
      # Изменения в otel-collector-config.yaml требуют перезапуска контейнера
      - ../../otel/collector.yaml:/etc/otel-collector-config.yaml

    ports:
      # ПОРТ 8888: метрики самого коллектора (Prometheus формат)
      # Можно мониторить производительность коллектора: curl localhost:8888/metrics
      - "8888:8888"

      # ПОРТ 4317: OTLP gRPC endpoint для приема логов от приложений
      # Go приложения отправляют логи именно на этот порт
      - "4317:4317"

    restart: unless-stopped
    networks:
      - microservices-net
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--spider", "http://localhost:8888/metrics"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 20s
      # Проверяем готовность коллектора через метрики
    depends_on:
      # ЗАВИСИМОСТЬ ОТ ELASTICSEARCH: коллектор отправляет данные в Elasticsearch,
      # поэтому должен запуститься после готовности Elasticsearch
      elasticsearch:
        condition: service_healthy
      prometheus:
        condition: service_healthy

  # ===================================================================
  # ELASTICSEARCH - ХРАНИЛИЩЕ ЛОГОВ
  # ===================================================================
  #
  # Elasticsearch - это распределенная поисковая система и база данных
  # для хранения и индексации логов. Все логи от приложений будут
  # сохраняться здесь в структурированном виде.

  elasticsearch:
    # Официальный образ Elasticsearch версии 9.0.4
    # Это стабильная версия с поддержкой ECS (Elastic Common Schema)
    image: elasticsearch:9.0.5

    # Имя контейнера для удобства обращения из других контейнеров
    container_name: elasticsearch

    environment:
      # РЕЖИМ ОДНОГО УЗЛА: Elasticsearch запускается как single-node кластер
      # В production обычно используется multi-node кластер для отказоустойчивости
      - discovery.type=single-node

      # ОТКЛЮЧЕНИЕ БЕЗОПАСНОСТИ: упрощает настройку для обучения
      # В production ОБЯЗАТЕЛЬНО включать аутентификацию и HTTPS
      - xpack.security.enabled=false

      # ИМЯ КЛАСТЕРА: логическое имя для группировки узлов
      - cluster.name=es-docker-cluster

    volumes:
      # ПОСТОЯННОЕ ХРАНЕНИЕ: данные Elasticsearch сохраняются на диске
      # Если контейнер перезапустится, данные не потеряются
      - es_data:/usr/share/elasticsearch/data

    ports:
      # ПОРТ 9200: REST API для взаимодействия с Elasticsearch
      # Через этот порт OTel Collector отправляет логи
      # Также можно делать запросы напрямую: curl localhost:9200/_search
      - "9200:9200"

    healthcheck:
      # ПРОВЕРКА ЗДОРОВЬЯ: Docker проверяет, что Elasticsearch готов к работе
      # Проверяем статус кластера через REST API
      test: [ "CMD-SHELL", "curl -fsS http://localhost:9200/_cluster/health >/dev/null" ]
      interval: 10s        # проверяем каждые 10 секунд
      timeout: 5s          # ждем ответ максимум 5 секунд
      retries: 20          # максимум 20 попыток (итого ~3 минуты)
      start_period: 30s    # ждем 30 секунд перед первой проверкой

    # АВТОМАТИЧЕСКИЙ ПЕРЕЗАПУСК: если контейнер упадет, Docker его перезапустит
    restart: unless-stopped

    networks:
      # ПОДКЛЮЧЕНИЕ К СЕТИ: все сервисы в одной сети могут общаться по имени
      - microservices-net

  # ===================================================================
  # KIBANA - ВЕБ-ИНТЕРФЕЙС ДЛЯ ПРОСМОТРА ЛОГОВ
  # ===================================================================
  #
  # Kibana предоставляет веб-интерфейс для поиска, фильтрации и
  # визуализации логов, хранящихся в Elasticsearch.

  kibana:
    # Официальный образ Kibana той же версии, что и Elasticsearch
    # Важно: версии Kibana и Elasticsearch должны совпадать!
    image: kibana:9.0.5

    container_name: kibana

    environment:
      # ПОДКЛЮЧЕНИЕ К ELASTICSEARCH: указываем адрес Elasticsearch
      # Используем имя контейнера 'elasticsearch' как DNS имя
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200

      # ОТКЛЮЧЕНИЕ БЕЗОПАСНОСТИ: соответствует настройкам Elasticsearch
      - XPACK_SECURITY_ENABLED=false

    depends_on:
      # ЗАВИСИМОСТЬ ОТ ELASTICSEARCH: Kibana запустится только после того,
      # как Elasticsearch будет healthy (пройдет healthcheck)
      elasticsearch:
        condition: service_healthy

    ports:
      # ПОРТ 5601: веб-интерфейс Kibana
      # Открываем в браузере http://localhost:5601 для просмотра логов
      - "5601:5601"

    healthcheck:
      # ПРОВЕРКА ЗДОРОВЬЯ KIBANA: проверяем готовность веб-интерфейса
      test: [ "CMD-SHELL", "curl -fsS http://localhost:5601/api/status >/dev/null" ]
      interval: 10s
      timeout: 5s
      retries: 20
      start_period: 30s

    restart: unless-stopped
    networks:
      - microservices-net

  # ===================================================================
  # KIBANA-INIT - АВТОМАТИЧЕСКАЯ НАСТРОЙКА DATA VIEW
  # ===================================================================
  #
  # Этот служебный контейнер автоматически создает Data View в Kibana
  # и настраивает его как default. Без Data View нельзя просматривать
  # логи в Kibana Discover.

  kibana-init:
    # Легкий Alpine Linux образ для выполнения скриптов инициализации
    image: alpine:3.20

    container_name: kibana-init

    depends_on:
      # ЖДЕМ ГОТОВНОСТИ KIBANA: запускаемся только после успешного healthcheck
      kibana:
        condition: service_healthy

    # КОМАНДА ИНИЦИАЛИЗАЦИИ: выполняется один раз при запуске контейнера
    command:
      - sh      # запускаем shell
      - -c      # выполняем команду из строки
      - |       # многострочная команда (YAML literal block)
        set -e  # останавливаем скрипт при первой ошибке

        # УСТАНОВКА УТИЛИТ: curl для HTTP запросов, jq для обработки JSON
        echo "Installing curl and jq..."
        apk add --no-cache curl jq >/dev/null 2>&1

        # ОЖИДАНИЕ ГОТОВНОСТИ KIBANA: дополнительная проверка доступности API
        echo "Waiting for Kibana..."
        for i in $$(seq 1 60); do  # максимум 60 попыток (2 минуты)
          if curl -sSf http://kibana:5601/api/status >/dev/null; then break; fi
          sleep 2  # ждем 2 секунды между попытками
        done

        # СОЗДАНИЕ DATA VIEW: настраиваем Kibana для просмотра логов
        echo "Creating Data View go-rocket-logs* ..."
        # Создаем Data View через Kibana API
        RESPONSE=$$(curl -sS -X POST http://kibana:5601/api/data_views/data_view \
          -H "kbn-xsrf: true" \
          -H "Content-Type: application/json" \
          --data '{"data_view":{"title":"go-rocket-logs*","name":"Go Rocket Logs","timeFieldName":"@timestamp","allowNoIndex":true},"override":true}' || echo "{}")

        # ИЗВЛЕЧЕНИЕ ID DATA VIEW: парсим JSON ответ для получения ID
        echo "Response: $$RESPONSE"
        DATA_VIEW_ID=$$(echo "$$RESPONSE" | jq -r ".data_view.id // empty")
        echo "Data View ID: $$DATA_VIEW_ID"

        # УСТАНОВКА КАК DEFAULT DATA VIEW: чтобы Kibana автоматически использовал его
        if [ -n "$$DATA_VIEW_ID" ]; then
          echo "Setting as default data view..."
          curl -sS -X POST http://kibana:5601/api/data_views/default \
            -H "kbn-xsrf: true" \
            -H "Content-Type: application/json" \
            -d "{\"data_view_id\": \"$$DATA_VIEW_ID\", \"force\": true}" || true
          echo "Default data view set successfully."
        else
          echo "Failed to get data view ID"
        fi
        echo "Done."

    networks:
      - microservices-net

    # НЕ ПЕРЕЗАПУСКАТЬ: это одноразовая задача инициализации
    restart: "no"

    # ===================================================================
    # OPENTELEMETRY COLLECTOR - ЦЕНТРАЛЬНЫЙ АГЕНТ СБОРА ЛОГОВ
    # ===================================================================
    #
    # OpenTelemetry Collector принимает логи от приложений, обрабатывает их
    # (добавляет метаданные, группирует в батчи) и отправляет в Elasticsearch.
    # Это промежуточный слой между приложениями и системой хранения.

  kafka: # Сервис Kafka
    image: confluentinc/cp-kafka:7.9.0 # Образ Kafka от компании Confluent, версия 7.9.0 (современная, с поддержкой KRaft)
    container_name: kafka # Явное имя контейнера, чтобы легче обращаться к нему
    ports:
      - "${KAFKA_EXTERNAL_PORT}:${KAFKA_EXTERNAL_PORT}" # Пробрасываем порт ${KAFKA_EXTERNAL_PORT} наружу — для доступа с хост-машины. Через этот порт будем подключаться клиентами с локального компьютера.

    env_file:
      - .env

    environment: # Список переменных окружения для конфигурации Kafka
      # === Основные параметры KRaft (Kafka без Zookeeper) ===
      KAFKA_KRAFT_MODE: "true" # Включаем работу в режиме KRaft (Kafka Raft Metadata Mode), без ZooKeeper.

      KAFKA_PROCESS_ROLES: controller,broker # Роль текущего процесса: одновременно controller (управляет metadata) и broker (обрабатывает сообщения).

      KAFKA_NODE_ID: 1 # Уникальный идентификатор ноды в кластере Kafka. В кластере должно быть уникальным для каждой ноды.

      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka:${KAFKA_CONTROLLER_PORT}"
      # Указываем список участников controller quorum для Raft —
      # в формате "ID@адрес:порт". У нас один узел: ID=1, адрес kafka, порт ${KAFKA_CONTROLLER_PORT}.
      # Это необходимо для распределённого консенсуса Kafka Metadata.

      # === Listeners (слушатели сети) ===
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:${KAFKA_INTERNAL_PORT},PLAINTEXT_EXTERNAL://0.0.0.0:${KAFKA_EXTERNAL_PORT},CONTROLLER://kafka:${KAFKA_CONTROLLER_PORT}
      # Настраиваем, на каких интерфейсах и портах Kafka будет слушать подключения.
      # - PLAINTEXT://0.0.0.0:${KAFKA_INTERNAL_PORT} — внутренний listener для контейнерной сети Docker.
      # - PLAINTEXT_EXTERNAL://0.0.0.0:${KAFKA_EXTERNAL_PORT} — внешний listener для подключения с локальной машины.
      # - CONTROLLER://kafka:${KAFKA_CONTROLLER_PORT} — служебный listener для связи контроллера и брокера внутри Kafka.

      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_EXTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
      # Определяем, какие протоколы безопасности используются для каждого listener.
      # В данном случае все listener'ы используют незашифрованный PLAINTEXT (для локальной разработки).

      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      # Указываем, какой listener используется для связи между брокерами (у нас один broker, но указать нужно).
      # PLAINTEXT — это внутренний listener на ${KAFKA_INTERNAL_PORT}.

      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      # Указываем, какой listener Kafka будет использовать для связи с controller (служебный трафик Raft).

      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:${KAFKA_INTERNAL_PORT},PLAINTEXT_EXTERNAL://localhost:${KAFKA_EXTERNAL_PORT},CONTROLLER://kafka:${KAFKA_CONTROLLER_PORT}
      # Важно! Kafka сообщает клиентам, по каким адресам её можно найти.
      # - Внутри Docker сети Kafka объявляет адрес kafka:${KAFKA_INTERNAL_PORT}.
      # - Для подключения с хоста Kafka объявляет localhost:${KAFKA_EXTERNAL_PORT}.
      # - Контроллеру Kafka сообщает адрес kafka:${KAFKA_CONTROLLER_PORT}.

      # === Поведение кластера ===
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      # Позволяет Kafka автоматически создавать топики при их первом использовании.

      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      # Количество реплик для системного топика offset'ов (у нас один брокер, поэтому 1).

      KAFKA_LOG_RETENTION_HOURS: 168
      # Хранить сообщения в топиках 168 часов (7 дней).

      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      # Устанавливаем задержку перед начальной балансировкой consumer group в 0 мс (ускоряем старт).

      CLUSTER_ID: "Mk3OEYBSD34fcwNTJENDM2Qk"
      # Уникальный идентификатор кластера Kafka.

    volumes:
      - kafka_data:/var/lib/kafka/data
      # Подключаем volume для хранения данных Kafka (логи и сообщения).
      # Это позволяет сохранять данные между перезапусками контейнера.

    healthcheck:
      test:
        [
          "CMD",
          "bash",
          "-c",
          "echo > /dev/tcp/localhost/${KAFKA_INTERNAL_PORT}",
        ]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 20s
      # Проверяем, доступен ли порт ${KAFKA_INTERNAL_PORT} внутри контейнера.
      # Если порт недоступен — Kafka считается неготовой.
      # start_period: 20s — ждём 20 секунд перед началом проверки.

    restart: unless-stopped
    # Автоматически перезапускаем контейнер при сбоях, но не при ручной остановке

    networks:
      - microservices-net
      # Подключаем контейнер к общей сети, в которой живут все сервисы микросервисной архитектуры

  kafka-ui: # Веб-интерфейс Kafka UI для управления брокером и топиками.
    image: provectuslabs/kafka-ui:v0.7.2
    container_name: kafka-ui
    ports:
      - "${KAFKA_UI_PORT}:8080"
      # Пробрасываем порт веб-интерфейса на хост-машину.

    env_file:
      - .env

    environment:
      KAFKA_CLUSTERS_0_NAME: "local-cluster"
      # Имя кластера, которое отобразится в UI.

      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: "kafka:${KAFKA_INTERNAL_PORT}"
      # Адрес Kafka брокера внутри контейнерной сети для подключения UI.

    depends_on:
      kafka:
        condition: service_healthy
      # Гарантируем, что Kafka UI стартует только после того, как Kafka станет healthy.

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/actuator/health"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 20s
      # Проверяем состояние UI по HTTP endpoint /actuator/health.
      # UI будет считаться healthy только после успешного ответа от сервера.

    restart: unless-stopped
    # Автоматически перезапускаем контейнер при сбоях, но не при ручной остановке

    networks:
      - microservices-net
      # Подключаем контейнер к общей сети, в которой живут все сервисы микросервисной архитектуры

  prometheus: # Сервис Prometheus для сбора и хранения метрик
    image: prom/prometheus:v3.3.1 # Используем официальный образ Prometheus последней стабильной версии
    container_name: prometheus # Явное имя контейнера
    ports:
      - "${PROMETHEUS_PORT}:9090" # Пробрасываем порт Prometheus на хост-машину
    volumes:
      - ../../prometheus/prometheus.yml:/etc/prometheus/prometheus.yml # Монтируем конфигурацию Prometheus
      - prometheus_data:/prometheus # Подключаем volume для хранения данных Prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml' # Указываем путь к файлу конфигурации
      - '--storage.tsdb.path=/prometheus' # Директория для хранения временных рядов
      - '--web.enable-lifecycle' # Включает HTTP API для управления Prometheus
      - '--web.enable-remote-write-receiver' # Включает поддержку remote write API для приема метрик
    healthcheck:
      test: [ "CMD", "wget", "--quiet", "--spider", "http://localhost:9090/-/healthy" ]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 20s
      # Проверяем готовность Prometheus по endpoint /-/healthy
    restart: unless-stopped
    # Автоматически перезапускаем контейнер при сбоях, но не при ручной остановке
    networks:
      - microservices-net
      # Подключаем контейнер к общей сети микросервисов

  grafana: # Сервис Grafana для визуализации метрик
    # image: grafana/grafana:12.0.0 # Используем официальный образ Grafana последней стабильной версии
    image: grafana/grafana:9.5.15 # Используем официальный образ Grafana последней стабильной версии
    container_name: grafana # Явное имя контейнера
    ports:
      - "${GRAFANA_PORT}:3000" # Пробрасываем порт Grafana на хост-машину
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin} # Имя пользователя администратора
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin} # Пароль администратора
      - GF_USERS_ALLOW_SIGN_UP=false # Запрещаем регистрацию новых пользователей
      - GF_PLUGINS_ALLOW_LOADING_UNSIGNED_PLUGINS=*
    volumes:
      - grafana_data:/var/lib/grafana # Подключаем volume для хранения данных Grafana
      - ./grafana/provisioning:/etc/grafana/provisioning # Монтируем директорию с настройками для автоматического провижининга
      - ./grafana/dashboards:/var/lib/grafana/dashboards # Монтируем директорию с дашбордами
    depends_on:
      prometheus:
        condition: service_healthy
      # Гарантируем, что Grafana стартует только после того, как Prometheus станет healthy
    healthcheck:
      test: [ "CMD", "wget", "--quiet", "--spider", "http://localhost:3000/api/health" ]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 20s
      # Проверяем готовность Grafana по endpoint /api/health
    restart: unless-stopped
    # Автоматически перезапускаем контейнер при сбоях, но не при ручной остановке
    networks:
      - microservices-net
      # Подключаем контейнер к общей сети микросервисов

volumes: # Раздел с определением томов
  prometheus_data:
  grafana_data:
  es_data:
  kafka_data: # Именованный том для хранения данных Kafka
  # Docker сам управляет этим хранилищем, данные сохраняются между перезапусками контейнеров.

networks:
  microservices-net:
    name: microservices-net
    external: false
    # ВАЖНО: здесь мы создаём сеть с именем microservices-net
    # Другие docker-compose файлы будут к ней подключаться с external: true
